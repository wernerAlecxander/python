# -----------------------------MEMORY MANAGEMENT (START)
# cleam memory everytime that a function become unnecessary:

import gc

def killer_func():
    result = []
    x = large_dataset_1()
    result.append(process_function(x))
    del x
    gc.collect()
    
    y = large_dataset_2()
    result.append(process_function(y))
    del y
    gc.collect()
    
    z = large_dataset_3()
    result.append(process_function(z))
    del z
    gc.collect()
    
    return result

# slice the function in small chunks to improve the memory management:

def process_large_dataset_1():
    x = large_dataset_1()
    result = process_function(x)
    return result

def process_large_dataset_2():
    y = large_dataset_2()
    result = process_function(y)
    return result

def process_large_dataset_3():
    z = large_dataset_3()
    result = process_function(z)
    return result
def killer_func():
    result = []
    result.append(process_large_dataset_1())
    result.append(process_large_dataset_2())
    result.append(process_large_dataset_3())
    return result

# process the data in smaller chunks:

def process_file_in_chunks(file_path, chunk_size=1024):
    with open(file_path, 'r') as file:
        while chunk := file.read(chunk_size):
            process_function(chunk)

# Use generators to process items one at a time instead of loading everything into memory:

def large_dataset_generator():
    for i in range(large_dataset_size):
        yield fetch_data(i)

def killer_func():
    result = []
    for data in large_dataset_generator():
        result.append(process_function(data))
    return result

#https://medium.com/@gabrielpelizzaro/how-to-safely-clean-memory-in-python-a-practical-guide-8e4dfa76d375
----------------------------------------------------------------
# import garbage collection module
import gc

# Create a list with a cyclic reference
my_list = []
my_list.append(my_list)

# Delete the list
del my_list

# Manually trigger garbage collection
collected = gc.collect()

# Verify memory release
print(f"Garbage collector collected {collected} objects.")

---------------

import gc
import tracemalloc

# Create a large list
data = [i for i in range(1000000)]

# Measure memory usage before deletion
tracemalloc.start()
snapshot1 = tracemalloc.take_snapshot()

# Delete the list explicitly
del data

# Force garbage collection to ensure memory release
gc.collect()

# Measure memory usage after deletion
snapshot2 = tracemalloc.take_snapshot()
stats = snapshot2.compare_to(snapshot1, 'lineno')
print(f"Memory released by deleting large list: {stats[0].size_diff / 10**6:.2f} MB")

-----------------

import gc
import tracemalloc

# Create a large dictionary
data_dict = {i: str(i) for i in range(10**6)}

# Measure memory usage before clearing
tracemalloc.start()
snapshot1 = tracemalloc.take_snapshot()

# Clear the dictionary explicitly
data_dict.clear()

# Force garbage collection to release memory
gc.collect()

# Measure memory usage after clearing
snapshot2 = tracemalloc.take_snapshot()
stats = snapshot2.compare_to(snapshot1, 'lineno')
print(f"Memory released by clearing dictionary: {stats[0].size_diff / 10**6:.2f} MB")

# https://www.geeksforgeeks.org/python/releasing-memory-in-python/

---------------------------------------------------------------------------
